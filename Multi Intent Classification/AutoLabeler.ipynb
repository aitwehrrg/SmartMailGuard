{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, num_labels, label, epoch1, epoch2, batch_size, drop):\n",
    "        # Load the pretrained BERT tokenizer and model for sequence classification\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)  # Set num_labels based on your dataset\n",
    "        self.drop = drop\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        train, test = self.split_df(df, self.drop)\n",
    "        self.training_args = TrainingArguments(\n",
    "                                output_dir='./results',          # output directory\n",
    "                                eval_strategy=\"epoch\",     # evaluate at each epoch\n",
    "                                per_device_train_batch_size=batch_size,   # batch size for training\n",
    "                                per_device_eval_batch_size=batch_size,    # batch size for evaluation\n",
    "                                num_train_epochs=epoch1,              # number of training epochs\n",
    "                                logging_dir='./logs',            # directory for storing logs\n",
    "                                logging_steps=10,\n",
    "                                save_strategy=\"epoch\",\n",
    "                                save_total_limit=1,\n",
    "                                report_to=[],\n",
    "                                push_to_hub=False,               # Avoid pushing the model to Hugging Face Hub\n",
    "                                load_best_model_at_end=True,\n",
    "                                weight_decay=0.01,\n",
    "                            )\n",
    "        self.trainer = Trainer(\n",
    "                            model=self.model,                         # the instantiated ðŸ¤— Transformers model\n",
    "                            args=self.training_args,                  # training arguments\n",
    "                            train_dataset=train,         # training dataset\n",
    "                            eval_dataset=test,            # evaluation dataset\n",
    "                            compute_metrics=compute_metrics,  # Pass the compute_metrics function\n",
    "                            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "                        )\n",
    "        \n",
    "        self.train(label)\n",
    "        \n",
    "        self.training_args.num_train_epochs = epoch2\n",
    "        \n",
    "    def split_df(self, df, drop):\n",
    "        # Convert DataFrame to Hugging Face Dataset\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "\n",
    "        # Optionally, split into train/test\n",
    "        train_test = dataset.train_test_split(test_size=drop)\n",
    "        train_dataset = train_test['train']\n",
    "        test_dataset = train_test['test']\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(examples['Text'], padding='max_length', truncation=True)\n",
    "\n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "        test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        # Rename 'Label' to 'labels' for the trainer to recognize it\n",
    "        train_dataset = train_dataset.map(lambda examples: {'labels': examples['Label']})\n",
    "        test_dataset = test_dataset.map(lambda examples: {'labels': examples['Label']})\n",
    "\n",
    "        # Remove unnecessary columns\n",
    "        train_dataset = train_dataset.remove_columns(['Text', 'Label'])\n",
    "        test_dataset = test_dataset.remove_columns(['Text', 'Label'])\n",
    "\n",
    "        # Set format for PyTorch\n",
    "        train_dataset.set_format('torch')\n",
    "        test_dataset.set_format('torch')\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def train(self, df):\n",
    "        self.model.train()\n",
    "        train, test = self.split_df(df, self.drop)\n",
    "        self.trainer.train_dataset = train\n",
    "        self.trainer.eval_dataset = test\n",
    "        \n",
    "        self.trainer.train()\n",
    "    \n",
    "    def predict_labels(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict labels for the given DataFrame containing a 'Text' column.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame with a 'Text' column.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing 'Text' and predicted 'Label'.\n",
    "        \"\"\"\n",
    "        # Check if 'Text' column exists\n",
    "        if 'Text' not in df.columns:\n",
    "            raise ValueError(\"Input DataFrame must contain a 'Text' column.\")\n",
    "\n",
    "        # Tokenize the Text column\n",
    "        inputs = self.tokenizer(df['Text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Move input tensors to the same device as the model\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Get the predicted labels\n",
    "        predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()  # Move predicted labels back to CPU\n",
    "\n",
    "        # Create a new DataFrame with Text and predicted Label\n",
    "        result_df = pd.DataFrame({\n",
    "            'Text': df['Text'],\n",
    "            'Label': predicted_labels\n",
    "        })\n",
    "\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dfs(dfs):\n",
    "    # Create a DataFrame to hold the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'Text': dfs[0]['Text'],  # Assuming all DataFrames have the same 'Text' column\n",
    "    })\n",
    "    \n",
    "    # Collect labels from all DataFrames for each text\n",
    "    for i in range(len(dfs)):\n",
    "        result_df[f'Label_{i}'] = dfs[i]['Label']\n",
    "    \n",
    "    # Identify rows with the same labels across all columns\n",
    "    same_label_mask = result_df.loc[:, result_df.columns.str.startswith('Label_')].nunique(axis=1) == 1\n",
    "    same_label_df = result_df[same_label_mask][['Text'] + result_df.columns.tolist()[1:]].copy()\n",
    "    same_label_df[\"Label\"] = same_label_df[\"Label_0\"]\n",
    "    \n",
    "    # Identify rows with different labels\n",
    "    different_label_df = result_df[~same_label_mask][['Text'] + result_df.columns.tolist()[1:]].copy()\n",
    "    \n",
    "    return same_label_df[['Text', 'Label']].reset_index(drop=True), different_label_df[[\"Text\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def autoLabel(label, unlabel, batch_size=0, max_time = 1, epoch1 = 10, epoch2 = 2, num_labels = 2, model_batch_size = 32, num_models = 1, drop=0.2):\n",
    "    '''\n",
    "    Input format:\n",
    "    label -> dataframe [columns: Text (sentence), Label (0, 1)]\n",
    "    unlabel -> dataframe [columns: Text (sentence)]\n",
    "    batch_size -> (int) number of rows in one batch\n",
    "    max_time -> (int in hours) After max_time has passed function returns the label and unlabel for checkpoint (function terminates)\n",
    "    epoch1 -> (int) number of epoch model should train initially\n",
    "    epoch2 -> (int) number of epoch model should train for each batch\n",
    "    ''' \n",
    "    \n",
    "    \n",
    "    models = [Model(num_labels=num_labels,\n",
    "                    label=label,\n",
    "                    epoch1=epoch1, \n",
    "                    epoch2=epoch2,\n",
    "                    batch_size=model_batch_size,\n",
    "                    drop=drop) \n",
    "              for i in range(num_models)]\n",
    "    \n",
    "    print(\"Model initialized\")\n",
    "    \n",
    "    if batch_size == 0:\n",
    "        batch_size = len(label)\n",
    "        \n",
    "    start = time.time()\n",
    "    max_time *= 3600\n",
    "    \n",
    "    unlabel = unlabel.sample(frac = 1).reset_index(drop=True)\n",
    "    batches = [unlabel[i:i+batch_size] for i in range(0, len(unlabel), batch_size)]\n",
    "    \n",
    "    unlabel = [pd.DataFrame(columns=['Text', 'Label'])]\n",
    "    \n",
    "    while(len(batches) and ((time.time() - start) < max_time)):\n",
    "        batch = batches.pop()\n",
    "        predicted_label = [model.predict_labels(batch) for model in models]\n",
    "        correct_prediction, incorrect_prediction = split_dfs(predicted_label)\n",
    "    \n",
    "        label = pd.concat([label, correct_prediction])\n",
    "        unlabel.append(incorrect_prediction)\n",
    "        \n",
    "        for model in models:\n",
    "            model.train(df)\n",
    "        \n",
    "        print(f\"Remaining batches: {len(batches)}, Labeled size: {len(label)}, New labels added: {len(correct_prediction)}\")\n",
    "        print(f'Time left: {max_time - (time.time() - start)}')\n",
    "        \n",
    "    if len(batches):\n",
    "        return label.reset_index(drop=True), pd.concat(batches + unlabel).reset_index(drop = True)\n",
    "    else:\n",
    "        return label.reset_index(drop=True), pd.concat(unlabel).reset_index(drop = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
