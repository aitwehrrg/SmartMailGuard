## Gradient vanishing or exploding problem

**Exploding gradient** : weights becomes too large

**Vanishing gradient** : weights becomes too small

**Solutions**:

1. Weight initialization:

W<sup>[l]</sup> = np.random.randn(shape) * np.sqrt(1 / n<sup>[l]</sup>)

For relu:

W<sup>[l]</sup> = np.random.randn(shape) * np.sqrt(2 / n<sup>[l]</sup>)


2. Normalization:

mean = np.sum(X, axis = 1, keepdim = True) / m

var<sup>2</sup> = np.sum(np.square(X), axis = 1, keepdim = True) / m

</br>

X = X - mean

X = X / var

## Gradient checking

> note: doesn't work for dropout regularization

dtheta<sub>approx</sub> = ( f ( theta + epsi ) - f( theta - epsi ) ) / 2 * epsi

concat: W<sup>[l]</sup> and b<sup>[l]</sup> to theta<sup>[l]</sup>

also dW<sup>[l]</sup> and db<sup>[l]</sup> to dtheta<sup>[l]</sup>

</br>

dtheta<sub>approx</sub><sup>[l]</sup> = 

( 
  J(theta<sup>[1]</sup>, theta<sup>[2]</sup>, .. theta<sup>[i] + epsi</sup>, .. theta<sup>[l]</sup>) + 
  
  J(theta<sup>[1]</sup>, theta<sup>[2]</sup>, .. theta<sup>[i] - epsi</sup>, .. theta<sup>[l]</sup>) ) / 2 * epsi


</br>

check = ||dtheta<sub>approx</sub>  - dtheta|| / (||dtheta<sub>approx</sub>|| + ||dtheta||)


> || a ||<sup>2</sup> = a<sub>1</sub><sup>2</sup> + a<sub>2</sub><sup>2</sup> + .. + a<sub>n</sub><sup>2</sup>


check <= 10<sup>-7</sup> for epsi = 10<sup>-7</sup>