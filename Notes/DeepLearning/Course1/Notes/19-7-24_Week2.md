## Binary Classification

### Logistic Regression

input = x (picture)

output = y (probability if picture is of cat or not)

Parameters: w (weights) , b (bias)

**Dimesions**

x : n

w : n

b : 1

**Formula (similar to linear regression but convert output to probability using sigmoid)**

y = sigmoid ( w<sup>T</sup> * x + b )

> sigmoid ( z ) =  1 / ( 1 + e <sup> -z </sup>)

### Loss function

example: y<sup>2</sup><sub>predict</sub> - y<sup>2</sup><sub>predict</sub>

Loss function for Logistic regression

L(y<sub>predict</sub>, y) = - ( y log (y<sub>predict</sub>) + (1 - y)log(1 - y<sub>predict</sub>) )

Cost function

J (w, b) = ( sum( L( y<sub>predict i</sub>, y ) for i = 1 to m ) ) / m

### Gradient Descent

Example : cost function is J(w)

    repeat:
      w -= alpha * dJ(w) / dw

notation: var dw = dJ(w) / dw

alpha = learning rate

### Backprop

use chain rule 

z = wx + b

a = sig( z )

L ( a, y)

J ( w, b) = sum L

### dvar

da = -y/a + (1 - y) / (1 - a)

dz = a (1 - a) * da

dz = a - y

dw = x dz

db = 1 dz = dz

****
for m inputs

{

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;z<sub>i</sub> = wx<sub>i</sub> + b

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a<sub>i</sub> = sig( z<sub>i</sub> )

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;J += J<sub>i</sub>

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dz<sub>i</sub> = a<sub>i</sub> - y

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dw += x<sub>i</sub> dz<sub>i</sub>

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;db += dz<sub>i</sub>

}

J /= m

dw /= m

db /= m

****





