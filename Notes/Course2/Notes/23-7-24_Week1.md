## Basic recipe for ML

### High bias

 - Bigger network 
 - Train longer
 - other architecture


### High Variance

 - more data
 - Regularization
 - other architecture

## Regularization

J (w, b) = 1 / m * sum ( L ( Y, A ) )  + lambd / 2m * Regulaization

### L1 regulaition

||w|| = sum ( |w| )

### L2 regulaition ( Frobenius norm )

||w||<sup>2</sup> = sum ( w<sup>2</sup> ) = sum ( w.T * w )

</br>

### update in backprop

dw<sup>[l]</sup> = (backprop) + lambd / m * W<sup>[l]</sup>

W<sup>[l]</sup> = W<sup>[l]</sup> - alpha * dW<sup>[l]</sup>

</br>
W<sup>[l]</sup> = ( 1 - alpha * lambd / m ) * W<sup>[l]</sup> - alpha * ( backprop )

> this is also called weight decay

### Dropout

removing random numbers of neurons 

a<sup>[l]</sup> = a<sup>[l]</sup> * (np.random.randn(a.shape[0], a.shape[1]) > keepdim)

a /= keepdim

