## Sequence to Sequence Architectures

It is like a combination of two Rnn

First Rnn: encodes the input sentence

Second Rnn: decodes the encoded output

</br>

First Rnn is many to one Rnn

Second Rnn is like a language model

</br>

### Beam Search

During decoding time instead on choosing next words greedily (choosing word with most probability)

We choose top B most probable next words and use that to check next most B most probable words from a set of (B * vocab size) and so on

### Length normalisation

When we multiply probabilities the result might become too small and cause underflow

To tackle this we take log as it is a increasing function so it will not affect the inequalities

We also normalize these probabilities by dividing it by T<sub>y</sub><sup>alpha</sup>

### Error Analysis
For wrong output

if P(Y) > P(A):

&nbsp;&nbsp;  We should try with bigger values of B

elif P(Y) <br P(A):

&nbsp;&nbsp; Try different Rnn


## Attention Model

Basic intuition is to assign weights to each input words for each output next word to be predicted in the sequence and use that as input along with previous output of Rnn/ LSTM to predict next word

To calculate these weights

First we create intermediate weights called energies

and take softmax of these energies

</br>

to calculate these energies

we use previously predicted word and one word of input (loop for all words) and pass it through shallow NN which outputs these energies



