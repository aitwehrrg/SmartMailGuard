## RNN

### Forward prop

a<sup>\<t></sup> = g<sub>1</sub>( W<sub>aa</sub> * a<sup>\<t-1></sup> + W<sub>ax</sub> * x<sup>\<t></sup> + b<sub>a</sub> )

> input for NN

y<sup>\<t></sup> = g<sub>2</sub>( W<sub>ya</sub> * a<sup>\<t></sup> + b<sub>y</sub> )

> output of NN

**Notation:**

W<sub>aa</sub> * a<sup>\<t-1></sup> + W<sub>ax</sub> * x<sup>\<t></sup> = W<sub>a</sub> * [a<sup>\<t-1></sup>, x<sup>\<t></sup>]

[a<sup>\<t-1></sup>, x<sup>\<t></sup>] stacked vertically

W<sub>a</sub> = [W<sub>aa</sub>, W<sub>ax</sub>] stacked horizontally



## GRU

memory cell: c<sup>\<t></sup>

candidate: c<sup>~\<t></sup> = tanh( W<sub>c</sub> @ [gamma<sub>r</sub><sup>\<t></sup> * c<sup>\<t-1></sup>, x<sup>\<t></sup>] + b<sub>c</sub> )

relevance: gamma<sub>r</sub><sup>\<t></sup> = sigmoid( W<sub>u</sub> @ [c<sup>\<t-1></sup>, x<sup>\<t></sup>] + b<sub>u</sub> )

gate: gamma<sub>u</sub><sup>\<t></sup> = sigmoid( W<sub>u</sub> @ [c<sup>\<t-1></sup>, x<sup>\<t></sup>] + b<sub>u</sub> )

c<sup>\<t></sup> = gamma<sub>u</sub><sup>\<t></sup> * c<sup>~\<t></sup> + (1 - gamma<sub>u</sub><sup>\<t></sup>) * c<sup>\<t-1></sup>

a<sup>\<t></sup> = c<sup>\<t></sup>

> note: * = element wise here


## LSTM

c<sup>~\<t></sup> = tanh( W<sub>c</sub> * [a<sup>\<t-1></sup>, x<sup>\<t></sup>] + b<sub>c</sub> )

gamma<sub>u</sub> : update gate = sigmoid( W<sub>u</sub> @ [a<sup>\<t-1></sup>, x<sup>\<t></sup>] + b<sub>u</sub> )

gamma<sub>f</sub> : forget gate = sigmoid( W<sub>f</sub> @ [a<sup>\<t-1></sup>, x<sup>\<t></sup>] + b<sub>f</sub> )

gamma<sub>o</sub> : output gate = sigmoid( W<sub>o</sub> @ [a<sup>\<t-1></sup>, x<sup>\<t></sup>] + b<sub>o</sub> )

c<sup>\<t></sup> = gamma<sub>u</sub> * c<sup>~\<t></sup> + gamma<sub>f</sub> * c<sup>\<t-1></sup>

a<sup>\<t></sup> = gamma<sub>o</sub> * tanh( c<sup>\<t></sup> )


## Bidirectional Rnn (BRNN)

y<sup>\<t></sup> = g(W<sub>y</sub> [a<sub>f</sub><sup>\<t></sup>, a<sub>b</sub><sup>\<t></sup>] + b<sub>y</sub>)

> note: two RNN second for reversed input


