{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class nn:\n",
    "\n",
    "  def __init__ (self, layerDim, activations, regulization=\"\", lambd=0, keep_prob=1, minibatch=0, descent=\"normal\", beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    self.layerDim = layerDim\n",
    "    self.activations = activations\n",
    "    self.layer = len(layerDim)\n",
    "\n",
    "    self.regulization = regulization\n",
    "    self.lambd = lambd\n",
    "\n",
    "    self.warr = []\n",
    "    self.barr = []\n",
    "    self.cachesZ = []\n",
    "    self.cacheA = []\n",
    "\n",
    "    self.cacheD = []\n",
    "    self.keep_prob = keep_prob\n",
    "\n",
    "    self.minibatch = minibatch\n",
    "\n",
    "    self.descent = descent\n",
    "    self.vCache = []\n",
    "    self.sCache = []\n",
    "    self.beta = beta\n",
    "    self.beta1 = beta1\n",
    "    self.beta2 = beta2\n",
    "    self.epsilon = epsilon\n",
    "    self.t = 1\n",
    "    \n",
    "    self.initParams()\n",
    "\n",
    "\n",
    "  # def __str__ (self):\n",
    "  #   return str(self.layerDim) + \" \" + str(self.activations)\n",
    "\n",
    "  def initParams (self):\n",
    "    for i in range(1, self.layer):\n",
    "        if self.activations[i-1] == 'relu':\n",
    "            self.warr.append(np.random.randn(self.layerDim[i], self.layerDim[i-1]) * np.sqrt(2. / self.layerDim[i-1]))\n",
    "        else:  # for sigmoid or tanh\n",
    "            self.warr.append(np.random.randn(self.layerDim[i], self.layerDim[i-1]) * np.sqrt(1. / self.layerDim[i-1]))\n",
    "        self.barr.append(np.zeros((self.layerDim[i], 1)))\n",
    "  \n",
    "  def sigmoid (self, Z):\n",
    "    Z = np.clip(Z, -500, 500) \n",
    "    return 1/(1+np.exp(-Z))\n",
    "  \n",
    "  def relu (self, Z):\n",
    "    return np.maximum(0, Z)\n",
    "  \n",
    "  def tanh (self, Z):\n",
    "    return np.tanh(Z)\n",
    "  \n",
    "  def activationsfunc (self, Z, activation):\n",
    "    if activation == 'sigmoid':\n",
    "      return self.sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "      return self.relu(Z)\n",
    "    elif activation == 'tanh':\n",
    "      return self.tanh(Z)\n",
    "    elif activation == 'softmax':\n",
    "      return self.softmax(Z)\n",
    "    else:\n",
    "      return Z\n",
    "  \n",
    "  def activationsDerivative(self, Z, activation):\n",
    "    if activation == 'sigmoid':\n",
    "      sig = self.sigmoid(Z)\n",
    "      return sig * (1 - sig)\n",
    "    elif activation == 'relu':\n",
    "      return (Z > 0).astype(Z.dtype)\n",
    "    elif activation == 'tanh':\n",
    "      return 1 - np.power(self.tanh(Z), 2)\n",
    "    else:\n",
    "      return 1\n",
    "  \n",
    "  def singleForward (self, A_prev, W, b, activation, layer=0):\n",
    "    Z = W @ A_prev + b\n",
    "    # A = Z\n",
    "    A = self.activationsfunc(Z, activation)\n",
    "    if self.regulization == \"dropout\" and layer != self.layer-2:\n",
    "      D = np.random.rand(A.shape[0], A.shape[1])\n",
    "      D = (D < self.keep_prob).astype(int)\n",
    "      A = A * D\n",
    "      A = A / self.keep_prob\n",
    "      self.cacheD.append(D)\n",
    "\n",
    "    self.cachesZ.append(Z)\n",
    "    self.cacheA.append(A)\n",
    "    \n",
    "    if self.descent == \"adam\":\n",
    "      self.vCache.append(np.zeros(W.shape))\n",
    "      self.sCache.append(np.zeros(W.shape))\n",
    "    elif self.descent == \"momentum\":\n",
    "      self.vCache.append(np.zeros(W.shape))\n",
    "    elif self.descent == \"rmsprop\":\n",
    "      self.vCache.append(np.zeros(W.shape))\n",
    "\n",
    "    return A\n",
    "\n",
    "  def forwardProp (self, X):\n",
    "    A = X\n",
    "    self.cachesZ = []\n",
    "    self.cacheA = []\n",
    "    self.vCache = []\n",
    "    self.sCache = []\n",
    "    self.cacheD = [X]\n",
    "    self.cacheA.append(A)\n",
    "\n",
    "    for i in range(self.layer-1):\n",
    "      A = self.singleForward(A, self.warr[i], self.barr[i], self.activations[i], i)\n",
    "    \n",
    "    return A\n",
    "  \n",
    "  def loss (self, Y, A):\n",
    "    r = 0\n",
    "    if self.regulization == 'l2':\n",
    "      for i in range(self.layer-1):\n",
    "        r += np.sum(np.square(self.warr[i]))\n",
    "      r *= self.lambd / (2 * Y.shape[1])\n",
    "\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    A = np.clip(A, 1e-10, 1 - 1e-10) \n",
    "\n",
    "    if self.activations[-1] == 'softmax':\n",
    "      return - np.sum(Y * np.log(A)) / m + r\n",
    "    else:\n",
    "      return - np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m + r\n",
    "  \n",
    "  def lossDerivative (self, Y, A):\n",
    "    m = Y.shape[1]\n",
    "    A = np.clip(A, 1e-10, 1 - 1e-10) \n",
    "    \n",
    "    if self.activations[-1] == 'softmax':\n",
    "      return A - Y\n",
    "    else:\n",
    "      return - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "  \n",
    "  def singleBackward (self, dA, W, b, Z, A_prev, activation):\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dZ = dA * self.activationsDerivative(Z, activation)\n",
    "    dW = 1/m * (dZ @ A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T @ dZ\n",
    "\n",
    "    if self.regulization == 'l2':\n",
    "      dW += self.lambd / m * W\n",
    "    elif self.regulization == 'dropout':\n",
    "      dA_prev = dA_prev * self.cacheD.pop()\n",
    "      dA_prev = dA_prev / self.keep_prob\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "  \n",
    "  def backwardProp (self, Y, A, learning_rate):\n",
    "    m = Y.shape[1]\n",
    "    dA = self.lossDerivative(Y, A)\n",
    "\n",
    "    for i in range(self.layer-1, 0, -1):\n",
    "      dA, dW, db = self.singleBackward(dA, self.warr[i-1], self.barr[i-1], self.cachesZ[i-1], self.cacheA[i-1], self.activations[i-1])\n",
    "\n",
    "      if self.descent == \"normal\":\n",
    "        self.warr[i-1] -= learning_rate * dW\n",
    "      elif self.descent == \"momentum\":\n",
    "        self.vCache[i-1] = self.beta * self.vCache[i-1] + (1 - self.beta) * dW\n",
    "        self.warr[i-1] -= learning_rate * self.vCache[i-1]\n",
    "      elif self.descent == \"rmsprop\":\n",
    "        self.sCache[i-1] = self.beta * self.sCache[i-1] + (1 - self.beta) * np.square(dW)\n",
    "        self.warr[i-1] -= learning_rate * dW / np.sqrt(self.sCache[i-1] + self.epsilon)\n",
    "      elif self.descent == \"adam\":\n",
    "        self.vCache[i-1] = self.beta1 * self.vCache[i-1] + (1 - self.beta1) * dW\n",
    "        self.sCache[i-1] = self.beta2 * self.sCache[i-1] + (1 - self.beta2) * np.square(dW)\n",
    "        vCorrected = self.vCache[i-1] / (1 - np.power(self.beta1, self.t))\n",
    "        sCorrected = self.sCache[i-1] / (1 - np.power(self.beta2, self.t))\n",
    "        self.t += 1\n",
    "        self.warr[i-1] -= learning_rate * vCorrected / np.sqrt(sCorrected + self.epsilon)\n",
    "\n",
    "      self.barr[i-1] -= learning_rate * db\n",
    "\n",
    "  def oneMinibatchTrain (self, X, Y, learning_rate):\n",
    "    A = self.predict(X)\n",
    "    self.backwardProp(Y, A, learning_rate)\n",
    "\n",
    "  def minibatchTrain (self, X, Y, learning_rate):\n",
    "    m = X.shape[1]\n",
    "    noBatch = m // self.minibatch\n",
    "    for i in range(noBatch):\n",
    "      X_batch = X[:, i*self.minibatch:(i+1)*self.minibatch]\n",
    "      Y_batch = Y[:, i*self.minibatch:(i+1)*self.minibatch]\n",
    "      self.oneMinibatchTrain(X_batch, Y_batch, learning_rate)\n",
    "    \n",
    "    if m % self.minibatch != 0:\n",
    "      X_batch = X[:, noBatch*self.minibatch:]\n",
    "      Y_batch = Y[:, noBatch*self.minibatch:]\n",
    "      self.oneMinibatchTrain(X_batch, Y_batch, learning_rate)\n",
    "\n",
    "  def train (self, X, Y, learning_rate, iterations, print_loss=False):\n",
    "    if self.minibatch == 0:\n",
    "      self.minibatch = X.shape[1]\n",
    "      \n",
    "    for i in range(iterations):\n",
    "      self.minibatchTrain(X, Y, learning_rate)\n",
    "      if i % 100 == 0 and print_loss:\n",
    "        print(f'Loss after {i} iterations: {self.loss(Y, self.predict(X))}')\n",
    "\n",
    "  def softmax (self, Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "  \n",
    "  def predict (self, X):\n",
    "    return self.forwardProp(X)\n",
    "  \n",
    "  def accuracy (self, X, Y):\n",
    "      A = self.predict(X)\n",
    "      return np.mean(np.argmax(Y, axis=0) == np.argmax(A, axis=0))\n",
    "\n",
    "  def precision (self, X, Y):\n",
    "      A = self.predict(X)\n",
    "      A = (A == A.max(axis=0, keepdims=1)).astype(int)\n",
    "      true_positive = np.sum((Y == 1) & (A == 1))\n",
    "      predicted_positive = np.sum(A == 1)\n",
    "      return true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "Y_train_mod = np.zeros((10, Y_train.shape[0]))\n",
    "\n",
    "for i in range(Y_train.shape[0]):\n",
    "  Y_train_mod[Y_train[i], i] = 1\n",
    "\n",
    "Y_test_mod = np.zeros((10, Y_test.shape[0]))\n",
    "\n",
    "for i in range(Y_test.shape[0]):\n",
    "  Y_test_mod[Y_test[i], i] = 1\n",
    "\n",
    "X_train_mod = X_train.reshape(X_train.shape[0], -1).T / 255\n",
    "X_test_mod = X_test.reshape(X_test.shape[0], -1).T / 255\n",
    "\n",
    "print(\"X shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iterations: 0.6052829466673281\n",
      "Loss after 100 iterations: 0.08779253177471888\n",
      "Loss after 200 iterations: 0.07130264782932624\n",
      "Loss after 300 iterations: 0.0664504397251217\n",
      "Loss after 400 iterations: 0.06442297284211652\n",
      "Loss after 500 iterations: 0.06328584392106205\n",
      "Loss after 600 iterations: 0.06255091512168405\n",
      "Loss after 700 iterations: 0.06200194839479548\n",
      "Loss after 800 iterations: 0.061613641596161124\n",
      "Loss after 900 iterations: 0.06133292306306901\n",
      "[[7.30821917e-06]\n",
      " [4.46864153e-06]\n",
      " [1.74598111e-05]\n",
      " [7.86026431e-02]\n",
      " [2.04468830e-09]\n",
      " [9.21255229e-01]\n",
      " [6.55566051e-07]\n",
      " [6.14481043e-05]\n",
      " [2.40335006e-05]\n",
      " [2.67520728e-05]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "model = nn([28* 28, 128, 64, 10], ['relu', 'relu', 'softmax'], minibatch=128, regulization=\"l2\", lambd=0.3, descent=\"momentum\", beta=0.9)\n",
    "\n",
    "\n",
    "model.train(X_train_mod, Y_train_mod, 0.1, 1000, True)\n",
    "\n",
    "print(model.predict(X_train_mod[:, 0:1]))\n",
    "print(Y_train_mod[:, 0:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9875\n",
      "Test accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model.accuracy(X_train_mod, Y_train_mod))\n",
    "print(\"Test accuracy:\", model.accuracy(X_test_mod, Y_test_mod))\n",
    "\n",
    "# print(\"Train precision:\", model.precision(X_train_mod, Y_train_mod))\n",
    "# print(\"Test precision:\", model.precision(X_test_mod, Y_test_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.30821917e-06]\n",
      " [4.46864153e-06]\n",
      " [1.74598111e-05]\n",
      " [7.86026431e-02]\n",
      " [2.04468830e-09]\n",
      " [9.21255229e-01]\n",
      " [6.55566051e-07]\n",
      " [6.14481043e-05]\n",
      " [2.40335006e-05]\n",
      " [2.67520728e-05]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_train_mod[:, 0:1]))\n",
    "print(Y_train_mod[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_test_mod[:, 0:1])\n",
    "i, j = np.unravel_index(p.argmax(), p.shape)\n",
    "print(i)\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
