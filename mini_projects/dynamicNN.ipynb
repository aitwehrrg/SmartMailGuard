{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class nn:\n",
    "  # layerDim = []\n",
    "  # warr = []\n",
    "  # barr = []\n",
    "  # activations = []\n",
    "  # cachesZ = []\n",
    "  # cacheA = []\n",
    "  # layer = 0\n",
    "\n",
    "  def __init__ (self, layerDim, activations):\n",
    "    self.layerDim = layerDim\n",
    "    self.activations = activations\n",
    "    self.layer = len(layerDim)\n",
    "    self.warr = []\n",
    "    self.barr = []\n",
    "    self.cachesZ = []\n",
    "    self.cacheA = []\n",
    "    self.initParams()\n",
    "\n",
    "  # def __str__ (self):\n",
    "  #   return str(self.layerDim) + \" \" + str(self.activations)\n",
    "\n",
    "  def initParams (self):\n",
    "    for i in range(1, self.layer):\n",
    "        if self.activations[i-1] == 'relu':\n",
    "            self.warr.append(np.random.randn(self.layerDim[i], self.layerDim[i-1]) * np.sqrt(2. / self.layerDim[i-1]))\n",
    "        else:  # for sigmoid or tanh\n",
    "            self.warr.append(np.random.randn(self.layerDim[i], self.layerDim[i-1]) * np.sqrt(1. / self.layerDim[i-1]))\n",
    "        self.barr.append(np.zeros((self.layerDim[i], 1)))\n",
    "  \n",
    "  def sigmoid (self, Z):\n",
    "    Z = np.clip(Z, -500, 500) \n",
    "    return 1/(1+np.exp(-Z))\n",
    "  \n",
    "  def relu (self, Z):\n",
    "    return np.maximum(0, Z)\n",
    "  \n",
    "  def tanh (self, Z):\n",
    "    return np.tanh(Z)\n",
    "  \n",
    "  def activationsfunc (self, Z, activation):\n",
    "    if activation == 'sigmoid':\n",
    "      return self.sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "      return self.relu(Z)\n",
    "    elif activation == 'tanh':\n",
    "      return self.tanh(Z)\n",
    "    else:\n",
    "      return Z\n",
    "  \n",
    "  def activationsDerivative(self, Z, activation):\n",
    "    if activation == 'sigmoid':\n",
    "      sig = self.sigmoid(Z)\n",
    "      return sig * (1 - sig)\n",
    "    elif activation == 'relu':\n",
    "      return (Z > 0).astype(Z.dtype)\n",
    "    elif activation == 'tanh':\n",
    "      return 1 - np.power(self.tanh(Z), 2)\n",
    "    else:\n",
    "      return 1\n",
    "  \n",
    "  def singleForward (self, A_prev, W, b, activation):\n",
    "    Z = W @ A_prev + b\n",
    "    # A = Z\n",
    "    A = self.activationsfunc(Z, activation)\n",
    "\n",
    "    self.cachesZ.append(Z)\n",
    "    self.cacheA.append(A)\n",
    "\n",
    "    return A\n",
    "\n",
    "  def forwardProp (self, X):\n",
    "    A = X\n",
    "    self.cachesZ = []\n",
    "    self.cacheA = []\n",
    "    self.cacheA.append(A)\n",
    "\n",
    "    for i in range(self.layer-1):\n",
    "      A = self.singleForward(A, self.warr[i], self.barr[i], self.activations[i])\n",
    "    \n",
    "    return A\n",
    "  \n",
    "  def loss (self, Y, A):\n",
    "    m = Y.shape[1]\n",
    "    A = np.clip(A, 1e-10, 1 - 1e-10) \n",
    "    return - np.sum(Y * np.log(A)) / m\n",
    "  \n",
    "  def lossDerivative (self, Y, A):\n",
    "    m = Y.shape[1]\n",
    "    return A - Y\n",
    "  \n",
    "  def singleBackward (self, dA, W, b, Z, A_prev, activation):\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dZ = dA * self.activationsDerivative(Z, activation)\n",
    "    dW = 1/m * (dZ @ A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T @ dZ\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "  \n",
    "  def backwardProp (self, Y, A, learning_rate):\n",
    "    m = Y.shape[1]\n",
    "    dA = self.lossDerivative(Y, A)\n",
    "\n",
    "    for i in range(self.layer-1, 0, -1):\n",
    "      dA, dW, db = self.singleBackward(dA, self.warr[i-1], self.barr[i-1], self.cachesZ[i-1], self.cacheA[i-1], self.activations[i-1])\n",
    "      self.warr[i-1] -= learning_rate * dW\n",
    "      self.barr[i-1] -= learning_rate * db\n",
    "\n",
    "  def train (self, X, Y, learning_rate, iterations, print_loss=False):\n",
    "    for i in range(iterations):\n",
    "      A = self.predict(X)\n",
    "      # print(\"forwardProp done\", A)\n",
    "      self.backwardProp(Y, A, learning_rate)\n",
    "      if i % 100 == 0 and print_loss:\n",
    "        print(f'Loss after {i} iterations: {self.loss(Y, A)}')\n",
    "\n",
    "  def softmax (self, Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "  \n",
    "  def predict (self, X):\n",
    "    return self.softmax(self.forwardProp(X))\n",
    "  \n",
    "  def accuracy (self, X, Y):\n",
    "      A = self.predict(X)\n",
    "      return np.mean(np.argmax(Y, axis=0) == np.argmax(A, axis=0))\n",
    "\n",
    "  def precision (self, X, Y):\n",
    "      A = self.predict(X)\n",
    "      A = (A == A.max(axis=0, keepdims=1)).astype(int)\n",
    "      true_positive = np.sum((Y == 1) & (A == 1))\n",
    "      predicted_positive = np.sum(A == 1)\n",
    "      return true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "Y_train_mod = np.zeros((10, Y_train.shape[0]))\n",
    "\n",
    "for i in range(Y_train.shape[0]):\n",
    "  Y_train_mod[Y_train[i], i] = 1\n",
    "\n",
    "Y_test_mod = np.zeros((10, Y_test.shape[0]))\n",
    "\n",
    "for i in range(Y_test.shape[0]):\n",
    "  Y_test_mod[Y_test[i], i] = 1\n",
    "\n",
    "X_train_mod = X_train.reshape(X_train.shape[0], -1).T / 255\n",
    "X_test_mod = X_test.reshape(X_test.shape[0], -1).T / 255\n",
    "\n",
    "print(\"X shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iterations: 2.3215299556773163\n",
      "Loss after 100 iterations: 0.42139840993879174\n",
      "Loss after 200 iterations: 0.32535695949093707\n",
      "Loss after 300 iterations: 0.2845333127264297\n",
      "Loss after 400 iterations: 0.25629660240643926\n",
      "Loss after 500 iterations: 0.23370530404867382\n",
      "Loss after 600 iterations: 0.2146618997189257\n",
      "Loss after 700 iterations: 0.19820013372335835\n",
      "Loss after 800 iterations: 0.1839268663914595\n",
      "Loss after 900 iterations: 0.17148610487039417\n",
      "[[2.29575172e-04]\n",
      " [4.59109633e-06]\n",
      " [1.10395291e-03]\n",
      " [1.24591314e-01]\n",
      " [2.74319382e-07]\n",
      " [8.73777113e-01]\n",
      " [5.15325900e-07]\n",
      " [1.29430721e-04]\n",
      " [9.78447375e-05]\n",
      " [6.53879835e-05]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "model = nn([28* 28, 128, 64, 10], ['relu', 'relu', ''])\n",
    "model.train(X_train_mod, Y_train_mod, 0.1, 1000, True)\n",
    "\n",
    "print(model.predict(X_train_mod[:, 0:1]))\n",
    "print(Y_train_mod[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.95465\n",
      "Test accuracy: 0.9506\n",
      "Train precision: 0.95465\n",
      "Test precision: 0.9506\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model.accuracy(X_train_mod, Y_train_mod))\n",
    "print(\"Test accuracy:\", model.accuracy(X_test_mod, Y_test_mod))\n",
    "\n",
    "print(\"Train precision:\", model.precision(X_train_mod, Y_train_mod))\n",
    "print(\"Test precision:\", model.precision(X_test_mod, Y_test_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.29575172e-04]\n",
      " [4.59109633e-06]\n",
      " [1.10395291e-03]\n",
      " [1.24591314e-01]\n",
      " [2.74319382e-07]\n",
      " [8.73777113e-01]\n",
      " [5.15325900e-07]\n",
      " [1.29430721e-04]\n",
      " [9.78447375e-05]\n",
      " [6.53879835e-05]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_train_mod[:, 0:1]))\n",
    "print(Y_train_mod[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_train_mod[:, 0:1])\n",
    "i, j = np.unravel_index(p.argmax(), p.shape)\n",
    "print(i)\n",
    "print(Y_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
