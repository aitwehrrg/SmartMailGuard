{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class nn:\n",
    "  # layerDim = []\n",
    "  # warr = []\n",
    "  # barr = []\n",
    "  # activations = []\n",
    "  # cachesZ = []\n",
    "  # cacheA = []\n",
    "  # layer = 0\n",
    "\n",
    "  def __init__ (self, layerDim, activations):\n",
    "    self.layerDim = layerDim\n",
    "    self.activations = activations\n",
    "    self.layer = len(layerDim)\n",
    "    self.warr = []\n",
    "    self.barr = []\n",
    "    self.cachesZ = []\n",
    "    self.cacheA = []\n",
    "    self.initParams()\n",
    "\n",
    "  # def __str__ (self):\n",
    "  #   return str(self.layerDim) + \" \" + str(self.activations)\n",
    "\n",
    "  def initParams (self):\n",
    "    for i in range(1, self.layer):\n",
    "        if self.activations[i-1] == 'relu':\n",
    "            self.warr.append(np.random.randn(self.layerDim[i], self.layerDim[i-1]) * np.sqrt(2. / self.layerDim[i-1]))\n",
    "        else:  # for sigmoid or tanh\n",
    "            self.warr.append(np.random.randn(self.layerDim[i], self.layerDim[i-1]) * np.sqrt(1. / self.layerDim[i-1]))\n",
    "        self.barr.append(np.zeros((self.layerDim[i], 1)))\n",
    "  \n",
    "  def sigmoid (self, Z):\n",
    "    Z = np.clip(Z, -500, 500) \n",
    "    return 1/(1+np.exp(-Z))\n",
    "  \n",
    "  def relu (self, Z):\n",
    "    return np.maximum(0, Z)\n",
    "  \n",
    "  def tanh (self, Z):\n",
    "    return np.tanh(Z)\n",
    "  \n",
    "  def activationsfunc (self, Z, activation):\n",
    "    if activation == 'sigmoid':\n",
    "      return self.sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "      return self.relu(Z)\n",
    "    elif activation == 'tanh':\n",
    "      return self.tanh(Z)\n",
    "    else:\n",
    "      return Z\n",
    "  \n",
    "  def activationsDerivative(self, Z, activation):\n",
    "    if activation == 'sigmoid':\n",
    "      sig = self.sigmoid(Z)\n",
    "      return sig * (1 - sig)\n",
    "    elif activation == 'relu':\n",
    "      return (Z > 0).astype(Z.dtype)\n",
    "    elif activation == 'tanh':\n",
    "      return 1 - np.power(self.tanh(Z), 2)\n",
    "    else:\n",
    "      return 1\n",
    "  \n",
    "  def singleForward (self, A_prev, W, b, activation):\n",
    "    Z = W @ A_prev + b\n",
    "    # A = Z\n",
    "    A = self.activationsfunc(Z, activation)\n",
    "\n",
    "    self.cachesZ.append(Z)\n",
    "    self.cacheA.append(A)\n",
    "\n",
    "    return A\n",
    "\n",
    "  def forwardProp (self, X):\n",
    "    A = X\n",
    "    self.cachesZ = []\n",
    "    self.cacheA = []\n",
    "    self.cacheA.append(A)\n",
    "\n",
    "    for i in range(self.layer-1):\n",
    "      A = self.singleForward(A, self.warr[i], self.barr[i], self.activations[i])\n",
    "    \n",
    "    return A\n",
    "  \n",
    "  def loss (self, Y, A):\n",
    "    m = Y.shape[1]\n",
    "    A = np.clip(A, 1e-10, 1 - 1e-10) \n",
    "    return - np.sum(Y * np.log(A)) / m\n",
    "  \n",
    "  def lossDerivative (self, Y, A):\n",
    "    m = Y.shape[1]\n",
    "    return A - Y\n",
    "  \n",
    "  def singleBackward (self, dA, W, b, Z, A_prev, activation):\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dZ = dA * self.activationsDerivative(Z, activation)\n",
    "    dW = 1/m * (dZ @ A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T @ dZ\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "  \n",
    "  def backwardProp (self, Y, A, learning_rate):\n",
    "    m = Y.shape[1]\n",
    "    dA = self.lossDerivative(Y, A)\n",
    "\n",
    "    for i in range(self.layer-1, 0, -1):\n",
    "      dA, dW, db = self.singleBackward(dA, self.warr[i-1], self.barr[i-1], self.cachesZ[i-1], self.cacheA[i-1], self.activations[i-1])\n",
    "      self.warr[i-1] -= learning_rate * dW\n",
    "      self.barr[i-1] -= learning_rate * db\n",
    "\n",
    "  def train (self, X, Y, learning_rate, iterations, print_loss=False):\n",
    "    for i in range(iterations):\n",
    "      A = self.predict(X)\n",
    "      # print(\"forwardProp done\", A)\n",
    "      self.backwardProp(Y, A, learning_rate)\n",
    "      if i % 100 == 0 and print_loss:\n",
    "        print(f'Loss after {i} iterations: {self.loss(Y, A)}')\n",
    "\n",
    "  def softmax (self, Z):\n",
    "    expZ = np.exp(Z - np.max(Z))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "  \n",
    "  def predict (self, X):\n",
    "    return self.softmax(self.forwardProp(X))\n",
    "  \n",
    "  def accuracy (self, X, Y):\n",
    "      A = self.predict(X)\n",
    "      return np.mean(np.argmax(Y, axis=0) == np.argmax(A, axis=0))\n",
    "\n",
    "  def precision (self, X, Y):\n",
    "      A = self.predict(X)\n",
    "      A = (A == A.max(axis=0, keepdims=1)).astype(int)\n",
    "      true_positive = np.sum((Y == 1) & (A == 1))\n",
    "      predicted_positive = np.sum(A == 1)\n",
    "      return true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "Y_train_mod = np.zeros((10, Y_train.shape[0]))\n",
    "\n",
    "for i in range(Y_train.shape[0]):\n",
    "  Y_train_mod[Y_train[i], i] = 1\n",
    "\n",
    "Y_test_mod = np.zeros((10, Y_test.shape[0]))\n",
    "\n",
    "for i in range(Y_test.shape[0]):\n",
    "  Y_test_mod[Y_test[i], i] = 1\n",
    "\n",
    "X_train_mod = X_train.reshape(X_train.shape[0], -1).T / 255\n",
    "X_test_mod = X_test.reshape(X_test.shape[0], -1).T / 255\n",
    "\n",
    "print(\"X shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iterations: 2.3772107816502763\n",
      "Loss after 100 iterations: 0.41208409515982625\n",
      "Loss after 200 iterations: 0.31308634555882364\n",
      "Loss after 300 iterations: 0.2724900857165541\n",
      "Loss after 400 iterations: 0.24516956241502333\n",
      "Loss after 500 iterations: 0.22393339518751015\n",
      "Loss after 600 iterations: 0.20653985463884642\n",
      "Loss after 700 iterations: 0.19190207791910324\n",
      "Loss after 800 iterations: 0.17925532469914393\n",
      "Loss after 900 iterations: 0.1680829867663212\n",
      "[[6.70614052e-05]\n",
      " [1.00744454e-05]\n",
      " [2.77252608e-04]\n",
      " [1.68067572e-02]\n",
      " [2.32999883e-08]\n",
      " [9.82628591e-01]\n",
      " [5.88207111e-07]\n",
      " [2.00228575e-04]\n",
      " [3.42832865e-06]\n",
      " [5.99519305e-06]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "model = nn([28* 28, 128, 64, 10], ['relu', 'relu', ''])\n",
    "model.train(X_train_mod, Y_train_mod, 0.1, 1000, True)\n",
    "\n",
    "print(model.predict(X_train_mod[:, 0:1]))\n",
    "print(Y_train_mod[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9549666666666666\n",
      "Test accuracy: 0.9539\n",
      "Train precision: 0.9549666666666666\n",
      "Test precision: 0.9539\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model.accuracy(X_train_mod, Y_train_mod))\n",
    "print(\"Test accuracy:\", model.accuracy(X_test_mod, Y_test_mod))\n",
    "\n",
    "print(\"Train precision:\", model.precision(X_train_mod, Y_train_mod))\n",
    "print(\"Test precision:\", model.precision(X_test_mod, Y_test_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.70614052e-05]\n",
      " [1.00744454e-05]\n",
      " [2.77252608e-04]\n",
      " [1.68067572e-02]\n",
      " [2.32999883e-08]\n",
      " [9.82628591e-01]\n",
      " [5.88207111e-07]\n",
      " [2.00228575e-04]\n",
      " [3.42832865e-06]\n",
      " [5.99519305e-06]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_train_mod[:, 0:1]))\n",
    "print(Y_train_mod[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_test_mod[:, 0:1])\n",
    "i, j = np.unravel_index(p.argmax(), p.shape)\n",
    "print(i)\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
