## Transformer

### Self Attention

to calculate attention we have 3 parameters 

Key (k), Query (q) and value (v)

we convert words into query vector by mutmal of Wq and word embeddings

these queries are like question whose answer is given but key vector

> key vector and value vector is calculated similarly

if key vector and query vector of a pair of words align their dot product will be large and vice versa

using this concept we calculate how surrounding words affect a target word and then update the embeddings of that word

word += attention

</br>

attention = softmax(QK<sup>T</sup> / sqrt(d<sub>k</sub>)) V

</br>

### Multi Headed attention

calculating and updating word embedding using self attention multiple times

### Encoding unit

it takes input word embedding modified with position encoding and pass it to multi headed attention whose out is then normalize and pass to a feed forward NN and then again to a normalization layer

this process repeats multiple times before pass to decoding unit

### Decoding unit

Encoding unit outputs key and value weights which will be used to calculate muliheaded attention.

output of the decoder is then passed on to a shallow NN and then to a softmax layer