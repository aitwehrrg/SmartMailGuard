## Hyperparamenters tuning

First preference:
- learning rate (alpha)

Second preference:
- mini batch size
- beta for gradient momentum
- hidden unit

Third preference:
- learning rate decay
- number of layers

Default only
- Adam (beta1, beta2, epsilon)

## Scale

### Learning Rate

0.00001 - 0.0001 - 0.001 - 0.01 - 0.1 - 1

r = -5 * np.random.rand()  
> [-5, 0]

a = 10<sup>r</sup>

# Beta in weighted average

0.9 - 0.09 - 0.009 - 0.0009

r = -2 * np.random.rand() - 1
> [-3, -1]

b = 1 - 10<sup>r</sup>


## Normalization

mean = np.sum(X, axis = 1, keepdim = True) / m

var<sup>2</sup> = np.sum(np.square(X), axis = 1, keepdim = True) / m

</br>

X = X - mean

X = X / var

</br>

### Batch norm

Z<sup>[l]\(1)</sup>, Z<sup>[l]\(2)</sup> ... Z<sup>[l]\(n)</sup>

mean = sum(Z<sup>l</sup>) column wise / m

var<sup>2</sup> = sum((Z - mean)<sup>2</sup>) / m

Z<sub>norm</sub> = (Z - mean)/ sqrt(var<sup>2</sup> + epsi)

> Z - mean : (nl, m) - (nl, 1)
> var : (nl, 1)

Zcap = gamma * Z<sub>norm</sub> + beta

> gamma and beta a learnable parameters of nn like w and b 

> gamma, beta : (nl, 1)

> norm just change the mean and variance of out data (inputs) 

>can get rid of bais parameter 


#### Drawback : no mean and var at test time as smaller batch of data

To tackle this we calculate the exponetially weighted average of mean and var of each layer

## SoftMax

exp = np.exp(Z)
softmax = exp/ sum(exp, axis = 1)

loss = y * log(A)

dZ = A - y

