## Optimization Algorithms

### Mini batch

Example: train dataset = 50,00,000

divide the dataset into smaller set 

preferably in power of 2 

Example code for 1024 batch size


    for it in range(iteration):
      for i in range(5000000 / 1024):
        gradient_descent(1_batch_of_size_1024)


## Exponentially Weighted average

v<sub>t</sub> =  beta * v<sub>t-1</sub> + (1 - beta) * theta<sub>t</sub>

**Bais correction**

v<sub>t</sub> / ( 1 - beta<sup>t</sup> )


### Gradient Descent with momentum

Find dW, db

V<sub>dW</sub> = beta * V<sub>dW</sub> + (1 - beta) * dW

V<sub>db</sub> = beta * V<sub>db</sub> + (1 - beta) * db

</br>

W = W - alpha * V<sub>dW</sub>

b = b - alpha * V<sub>db</sub>

## RMS prop

Find dW, db

S<sub>dW</sub> = beta * S<sub>dW</sub> + (1 - beta) * dW<sup>2</sup>

S<sub>db</sub> = beta * S<sub>db</sub> + (1 - beta) * db<sup>2</sup>
> note: element wise square

</br>
W = W - alpha * dW / sqrt( S<sub>dW</sub> )

b = b - alpha * db / sqrt( S<sub>db</sub> )

****

## Adam optimization (Adapted moment estimation)

V<sub>dW</sub>, V<sub>db</sub>, S<sub>dW</sub>, S<sub>db</sub> = 0

</br>

V<sub>dW</sub> = beta<sub>1</sub> * V<sub>dW</sub> + (1 - beta<sub>1</sub>) * dW

V<sub>db</sub> = beta<sub>1</sub> * V<sub>db</sub> + (1 - beta<sub>1</sub>) * db

V<sub>dW</sub> = V<sub>dW</sub> / (1 - beta<sub>1</sub><sup>t</sup>)

V<sub>db</sub> = V<sub>db</sub> / (1 - beta<sub>1</sub><sup>t</sup>)

</br>

S<sub>dW</sub> = beta<sub>2</sub> * S<sub>dW</sub> + (1 - beta<sub>2</sub>) * dW<sup>2</sup>

S<sub>db</sub> = beta<sub>2</sub> * S<sub>db</sub> + (1 - beta<sub>2</sub>) * db<sup>2</sup>

S<sub>dW</sub> = S<sub>dW</sub> / (1 - beta<sub>2</sub><sup>t</sup>)

S<sub>db</sub> = S<sub>db</sub> / (1 - beta<sub>2</sub><sup>t</sup>)

</br>

W = W - alpha * V<sub>dW</sub> / sqrt( S<sub>dW</sub> +epsi )

b = b - alpha * V<sub>db</sub> / sqrt( S<sub>db</sub> +epsi )

## Learning Rate decay

1 epoch = finished one iteration of training set

alpha = ( 1 / ( 1 + decay_rate * epoch_nums) ) * a<sub>o</sub>

</br>

**Method 2:**

alpha = (no. less than 1)<sup>epoch_num</sup> * a<sub>o</sub>

Method 3:
 
alpha = l / sqrt( epoch_nums ) * a<sub>o</sub>

